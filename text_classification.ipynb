{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Text Classification\n",
    "### Vincent Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20 newsgroups text dataset was applied in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: ../input/20-newsgroup-sklearn/20news-bydate_py3*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import get_data_home\n",
    "\n",
    "data_home = get_data_home()\n",
    "twenty_home = os.path.join(data_home, \"20news_home\")\n",
    "\n",
    "if not os.path.exists(data_home):\n",
    "    os.makedirs(data_home)\n",
    "    \n",
    "if not os.path.exists(twenty_home):\n",
    "    os.makedirs(twenty_home)\n",
    "    \n",
    "!cp ../input/20-newsgroup-sklearn/20news-bydate_py3* /tmp/scikit_learn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/wangziwen/scikit_learn_data'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/wangziwen/scikit_learn_data/20news_home'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://qwone.com/~jason/20Newsgroups/\n",
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, download_if_missing=False)\n",
    "texts = dataset.data # Extract text\n",
    "target = dataset.target # Extract target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to tokenize the text before we can feed it into a neural network. This tokenization process will also remove some of the features of the original text, such as all punctuation or words that are less common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "18846\n",
      "157\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "10\n",
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print (len(texts))\n",
    "print (len(target))\n",
    "print (len(texts[0].split()))\n",
    "print (texts[0])\n",
    "print (target[0])\n",
    "print (dataset.target_names[target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to specify the size of our vocabulary. Words that are less frequent will get removed. In this case we want to retain the 20,000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1595, 151, 82, 20, 13, 7, 122, 706, 82, 50, 13]]\n",
      "18846\n",
      "160\n",
      "[14, 19415, 455, 559, 15, 29, 2552, 1240, 5609, 33, 322, 767, 2175, 2121, 871, 1343, 32, 251, 88, 77, 84, 12087, 455, 559, 15, 7, 122, 228, 63, 3, 2552, 1240, 20, 517, 3490, 50, 1, 1393, 3, 61, 437, 3, 1507, 50, 1, 1302, 2552, 3027, 3, 1, 2701, 309, 7, 122, 243, 16334, 175, 5, 4, 243, 19416, 268, 7, 122, 194, 2, 296, 37, 337, 2, 369, 4389, 22, 4, 243, 3, 7286, 12, 1, 2552, 349, 30, 20, 1502, 137, 2701, 1382, 90, 7, 397, 5987, 74, 2025, 13, 130, 56, 8, 140, 215, 90, 93, 1457, 770, 1963, 56, 8, 97, 4, 308, 9186, 1857, 2, 1306, 6, 1, 2327, 6760, 115, 348, 5987, 21, 4, 308, 3, 1857, 6, 1, 365, 658, 3, 467, 185, 1, 2552, 20, 194, 2, 1985, 1, 66, 3, 3215, 608, 7, 26, 132, 8755, 19, 2, 131, 1, 3280, 2000, 1, 1151, 1457, 770, 283, 2552, 1222]\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.texts_to_sequences(['hello world, how are you? I am fine, how about you?']))\n",
    "\n",
    "print (len(sequences))\n",
    "print (len(sequences[0]))\n",
    "print (sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179,209 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our text is now converted to sequences of numbers. It makes sense to convert some of those sequences back into text to check what the tokenization did to our text. To this end we create an inverse index that maps numbers to words while the tokenizer maps words to numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from ratnam andrew cmu edu subject pens fans reactions organization post office carnegie mellon pittsburgh pa lines 12 nntp posting host po4 andrew cmu edu i am sure some of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils actually i am bit puzzled too and a bit relieved however i am going to put an end to non relief with a bit of praise for the pens man they are killing those devils worse than i thought jagr just showed you why he is much better than his regular season stats he is also a lot fo fun to watch in the playoffs bowman should let jagr have a lot of fun in the next couple of games since the pens are going to beat the out of jersey anyway i was very disappointed not to see the islanders lose the final regular season game pens rule "
     ]
    }
   ],
   "source": [
    "# Create inverse index mapping numbers to words\n",
    "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Print out text again\n",
    "for w in sequences[0]:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring text length\n",
    "### Let's ensure all sequences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292.4769712405816, 666.93290630508761)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "avg,std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can see, the average text is about 300 words long. However, the standard deviation is quite large which indicates that some texts are much much longer. If some user decided to write an epic novel in the newsgroup it would massively slow down training. So for speed purposes we will restrict sequence length to 100 words. You should try out some different sequence lengths and experiment with processing time and accuracy gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 3]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences([[1,2,3]], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19416,   268,     7,   122,   194,     2,   296,    37,   337,\n",
       "           2,   369,  4389,    22,     4,   243,     3,  7286,    12,\n",
       "           1,  2552,   349,    30,    20,  1502,   137,  2701,  1382,\n",
       "          90,     7,   397,  5987,    74,  2025,    13,   130,    56,\n",
       "           8,   140,   215,    90,    93,  1457,   770,  1963,    56,\n",
       "           8,    97,     4,   308,  9186,  1857,     2,  1306,     6,\n",
       "           1,  2327,  6760,   115,   348,  5987,    21,     4,   308,\n",
       "           3,  1857,     6,     1,   365,   658,     3,   467,   185,\n",
       "           1,  2552,    20,   194,     2,  1985,     1,    66,     3,\n",
       "        3215,   608,     7,    26,   132,  8755,    19,     2,   131,\n",
       "           1,  3280,  2000,     1,  1151,  1457,   770,   283,  2552,  1222], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning labels into One-Hot encodings\n",
    "### Labels can quickly be encoded into one-hot vectors with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (18846, 100)\n",
      "Shape of labels: (18846, 20)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(target))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "#Glove Embeddings Pre trained model can be download: https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation/downloads/glove-global-vectors-for-word-representation.zip/1\n",
    "glove_dir = 'glove-global-vectors-for-word-representation' # This is the folder with the dataset\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.043084    0.53232998  0.54254001 -0.076952   -0.29673001  0.52986002\n",
      "  0.21379     0.15789001 -0.39520001 -0.91889    -0.65850002  0.68706\n",
      "  0.10821    -0.10694    -0.34009999  1.04400003  0.12774999  0.51156998\n",
      "  0.60314     0.71366    -0.53740001  0.37737     0.12186     0.60891002\n",
      "  0.50107002  2.02150011 -0.47318     0.46952999  0.12542     0.60206997\n",
      "  0.11007     0.37586999  1.01370001 -0.24779999  0.65748     0.12801\n",
      " -0.57647002 -0.25753999  0.62426001  0.010864   -0.40680999  0.16173001\n",
      " -0.84694999 -0.24603     0.29078001  0.85460001 -0.067021    0.69331002\n",
      " -0.71544999 -0.25184    -0.74741    -0.26506999  0.48730001  0.41991001\n",
      " -0.86741    -0.52350003 -0.44773999 -0.044584    0.033836    0.29909\n",
      "  0.73754001  0.81651002  0.69431001  0.80453002  0.29276001 -0.025244\n",
      " -0.30452999 -0.34329     0.11933    -0.29655001  0.1072     -0.18945999\n",
      "  0.18501    -0.75480002 -0.25628     0.34437999 -0.016743    0.0040503\n",
      "  0.39342001  0.99404001 -0.32159001 -0.49434     0.41707999 -0.011019\n",
      " -0.16613001 -0.20839     0.28152001 -0.82995999  0.79838997  0.61645001\n",
      "  0.31536999 -0.27629    -0.54592001  0.23026     0.023473   -0.15933999\n",
      " -1.43889999 -0.75358999  0.51490003 -0.52552003]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print (embeddings_index['frog'])\n",
    "print (len(embeddings_index['frog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12497\n",
      "6.79435\n"
     ]
    }
   ],
   "source": [
    "# https://nlp.stanford.edu/projects/glove/\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['toad']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wangziwen/anaconda/envs/python2.7/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 128)           38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,172,564\n",
      "Trainable params: 172,564\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable = False))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15076 samples, validate on 3770 samples\n",
      "Epoch 1/20\n",
      "15076/15076 [==============================] - 10s 657us/step - loss: 0.0841 - acc: 0.9692 - val_loss: 0.0904 - val_acc: 0.9682\n",
      "Epoch 2/20\n",
      "15076/15076 [==============================] - 9s 592us/step - loss: 0.0709 - acc: 0.9738 - val_loss: 0.0890 - val_acc: 0.9691\n",
      "Epoch 3/20\n",
      "15076/15076 [==============================] - 9s 622us/step - loss: 0.0589 - acc: 0.9782 - val_loss: 0.0925 - val_acc: 0.9696\n",
      "Epoch 4/20\n",
      "15076/15076 [==============================] - 10s 638us/step - loss: 0.0488 - acc: 0.9822 - val_loss: 0.0922 - val_acc: 0.9692\n",
      "Epoch 5/20\n",
      "15076/15076 [==============================] - 10s 654us/step - loss: 0.0378 - acc: 0.9863 - val_loss: 0.0999 - val_acc: 0.9692\n",
      "Epoch 6/20\n",
      "15076/15076 [==============================] - 10s 651us/step - loss: 0.0287 - acc: 0.9897 - val_loss: 0.1109 - val_acc: 0.9671\n",
      "Epoch 7/20\n",
      "15076/15076 [==============================] - 10s 660us/step - loss: 0.0208 - acc: 0.9928 - val_loss: 0.1173 - val_acc: 0.9675\n",
      "Epoch 8/20\n",
      "15076/15076 [==============================] - 10s 668us/step - loss: 0.0171 - acc: 0.9942 - val_loss: 0.1387 - val_acc: 0.9649\n",
      "Epoch 9/20\n",
      "15076/15076 [==============================] - 10s 667us/step - loss: 0.0150 - acc: 0.9948 - val_loss: 0.1505 - val_acc: 0.9638\n",
      "Epoch 10/20\n",
      "15076/15076 [==============================] - 10s 659us/step - loss: 0.0142 - acc: 0.9950 - val_loss: 0.1643 - val_acc: 0.9647\n",
      "Epoch 11/20\n",
      "15076/15076 [==============================] - 10s 658us/step - loss: 0.0131 - acc: 0.9955 - val_loss: 0.1682 - val_acc: 0.9645\n",
      "Epoch 12/20\n",
      "15076/15076 [==============================] - 10s 649us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.1692 - val_acc: 0.9646\n",
      "Epoch 13/20\n",
      "15076/15076 [==============================] - 10s 664us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.1737 - val_acc: 0.9644\n",
      "Epoch 14/20\n",
      "15076/15076 [==============================] - 10s 650us/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.1735 - val_acc: 0.9636\n",
      "Epoch 15/20\n",
      "15076/15076 [==============================] - 10s 650us/step - loss: 0.0102 - acc: 0.9966 - val_loss: 0.1761 - val_acc: 0.9648\n",
      "Epoch 16/20\n",
      "15076/15076 [==============================] - 10s 678us/step - loss: 0.0077 - acc: 0.9973 - val_loss: 0.1819 - val_acc: 0.9625\n",
      "Epoch 17/20\n",
      "15076/15076 [==============================] - 10s 670us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.1972 - val_acc: 0.9622\n",
      "Epoch 18/20\n",
      "15076/15076 [==============================] - 10s 687us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.2017 - val_acc: 0.9633\n",
      "Epoch 19/20\n",
      "15076/15076 [==============================] - 10s 678us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.2054 - val_acc: 0.9636\n",
      "Epoch 20/20\n",
      "15076/15076 [==============================] - 10s 666us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 0.2007 - val_acc: 0.9614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13a6e4e80>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data, labels, validation_split=0.2, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model achieves more than 95% accuracy on the validation set in only 2 epochs. Systems like these can be used to assign emails in customer support centers, suggest responses, or classify other forms of text like invoices which need to be assigned to an department. Let's take a look at how our model classified one of the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 i'm posting this for a friend but you can e mail questions to me at cc bellcore com however the best way to get your questions answered is to call the phone number listed for sale 1991 corrado 2 2 coupe low mileage approx 28 000 miles 5 speed manual 7 speaker factory stereo system new all weather 205 sun roof ac red speed activated extra set of tires equipped with factory winter package heated seats mirrors and security system with 2 all records documentation service car mint condition must sacrifice at 11 000 or best offer call 908 "
     ]
    }
   ],
   "source": [
    "example = data[450] # get the tokens\n",
    "\n",
    "# Print tokens as text\n",
    "for w in example:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction\n",
    "pred = model.predict(example.reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'misc.forsale'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output predicted category\n",
    "dataset.target_names[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
